Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
23/06/17 12:06:04 INFO SparkContext: Running Spark version 3.2.3
23/06/17 12:06:07 INFO ResourceUtils: ==============================================================
23/06/17 12:06:07 INFO ResourceUtils: No custom resources configured for spark.driver.
23/06/17 12:06:07 INFO ResourceUtils: ==============================================================
23/06/17 12:06:07 INFO SparkContext: Submitted application: SparkByExamples.com
23/06/17 12:06:08 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/06/17 12:06:08 INFO ResourceProfile: Limiting resource is cpu
23/06/17 12:06:08 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/06/17 12:06:09 INFO SecurityManager: Changing view acls to: kotayaswanth
23/06/17 12:06:09 INFO SecurityManager: Changing modify acls to: kotayaswanth
23/06/17 12:06:09 INFO SecurityManager: Changing view acls groups to: 
23/06/17 12:06:09 INFO SecurityManager: Changing modify acls groups to: 
23/06/17 12:06:09 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kotayaswanth); groups with view permissions: Set(); users  with modify permissions: Set(kotayaswanth); groups with modify permissions: Set()
23/06/17 12:06:43 INFO Utils: Successfully started service 'sparkDriver' on port 50672.
23/06/17 12:06:43 INFO SparkEnv: Registering MapOutputTracker
23/06/17 12:06:44 INFO SparkEnv: Registering BlockManagerMaster
23/06/17 12:06:45 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/06/17 12:06:45 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/06/17 12:06:45 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/06/17 12:06:45 INFO DiskBlockManager: Created local directory at C:\Users\kotayaswanth\AppData\Local\Temp\blockmgr-58e8cb99-67ce-4165-bf00-bb8c04d11cd4
23/06/17 12:06:46 INFO MemoryStore: MemoryStore started with capacity 883.5 MiB
23/06/17 12:06:46 INFO SparkEnv: Registering OutputCommitCoordinator
23/06/17 12:06:49 INFO Utils: Successfully started service 'SparkUI' on port 4040.
23/06/17 12:06:50 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://LAPTOP-SPPPB9PB:4040
23/06/17 12:06:52 INFO Executor: Starting executor ID driver on host LAPTOP-SPPPB9PB
23/06/17 12:06:52 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50700.
23/06/17 12:06:52 INFO NettyBlockTransferService: Server created on LAPTOP-SPPPB9PB:50700
23/06/17 12:06:52 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/06/17 12:06:52 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, LAPTOP-SPPPB9PB, 50700, None)
23/06/17 12:06:52 INFO BlockManagerMasterEndpoint: Registering block manager LAPTOP-SPPPB9PB:50700 with 883.5 MiB RAM, BlockManagerId(driver, LAPTOP-SPPPB9PB, 50700, None)
23/06/17 12:06:52 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, LAPTOP-SPPPB9PB, 50700, None)
23/06/17 12:06:52 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, LAPTOP-SPPPB9PB, 50700, None)
read csv files base on wildcard character
23/06/17 12:06:56 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
23/06/17 12:06:58 INFO SharedState: Warehouse path is 'file:/C:/capstone/capstonemarketanalysis/spark-warehouse'.
23/06/17 12:07:04 INFO InMemoryFileIndex: It took 309 ms to list leaf files for 1 paths.
23/06/17 12:07:05 INFO InMemoryFileIndex: It took 6 ms to list leaf files for 1 paths.
23/06/17 12:07:08 WARN ProcfsMetricsGetter: Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
23/06/17 12:07:24 INFO FileSourceStrategy: Pushed Filters: 
23/06/17 12:07:24 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)
23/06/17 12:07:24 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
23/06/17 12:07:28 INFO CodeGenerator: Code generated in 1781.6639 ms
23/06/17 12:07:28 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 337.5 KiB, free 883.2 MiB)
23/06/17 12:07:29 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.4 KiB, free 883.1 MiB)
23/06/17 12:07:29 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on LAPTOP-SPPPB9PB:50700 (size: 32.4 KiB, free: 883.5 MiB)
23/06/17 12:07:29 INFO SparkContext: Created broadcast 0 from csv at Demo.scala:18
23/06/17 12:07:29 INFO FileSourceScanExec: Planning scan with bin packing, max size: 9573254 bytes, open cost is considered as scanning 4194304 bytes.
23/06/17 12:07:30 INFO SparkContext: Starting job: csv at Demo.scala:18
23/06/17 12:07:30 INFO DAGScheduler: Got job 0 (csv at Demo.scala:18) with 1 output partitions
23/06/17 12:07:30 INFO DAGScheduler: Final stage: ResultStage 0 (csv at Demo.scala:18)
23/06/17 12:07:30 INFO DAGScheduler: Parents of final stage: List()
23/06/17 12:07:30 INFO DAGScheduler: Missing parents: List()
23/06/17 12:07:30 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at Demo.scala:18), which has no missing parents
23/06/17 12:07:30 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 11.6 KiB, free 883.1 MiB)
23/06/17 12:07:30 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 883.1 MiB)
23/06/17 12:07:30 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on LAPTOP-SPPPB9PB:50700 (size: 5.8 KiB, free: 883.5 MiB)
23/06/17 12:07:30 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1478
23/06/17 12:07:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at Demo.scala:18) (first 15 tasks are for partitions Vector(0))
23/06/17 12:07:30 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
23/06/17 12:07:31 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (LAPTOP-SPPPB9PB, executor driver, partition 0, PROCESS_LOCAL, 4883 bytes) taskResourceAssignments Map()
23/06/17 12:07:31 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
23/06/17 12:07:32 INFO FileScanRDD: Reading File path: file:///C:/capstone/capstonemarketanalysis/data/Source.csv, range: 0-5378950, partition values: [empty row]
23/06/17 12:07:32 INFO CodeGenerator: Code generated in 55.226 ms
23/06/17 12:07:32 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1696 bytes result sent to driver
23/06/17 12:07:32 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1172 ms on LAPTOP-SPPPB9PB (executor driver) (1/1)
23/06/17 12:07:32 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
23/06/17 12:07:32 INFO DAGScheduler: ResultStage 0 (csv at Demo.scala:18) finished in 1.741 s
23/06/17 12:07:32 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
23/06/17 12:07:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
23/06/17 12:07:32 INFO DAGScheduler: Job 0 finished: csv at Demo.scala:18, took 1.881033 s
23/06/17 12:07:32 INFO CodeGenerator: Code generated in 33.592 ms
23/06/17 12:07:32 INFO FileSourceStrategy: Pushed Filters: 
23/06/17 12:07:32 INFO FileSourceStrategy: Post-Scan Filters: 
23/06/17 12:07:32 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
23/06/17 12:07:32 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 337.5 KiB, free 882.8 MiB)
23/06/17 12:07:32 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 32.4 KiB, free 882.8 MiB)
23/06/17 12:07:32 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on LAPTOP-SPPPB9PB:50700 (size: 32.4 KiB, free: 883.4 MiB)
23/06/17 12:07:32 INFO SparkContext: Created broadcast 2 from csv at Demo.scala:18
23/06/17 12:07:32 INFO FileSourceScanExec: Planning scan with bin packing, max size: 9573254 bytes, open cost is considered as scanning 4194304 bytes.
23/06/17 12:07:33 INFO FileSourceStrategy: Pushed Filters: 
23/06/17 12:07:33 INFO FileSourceStrategy: Post-Scan Filters: 
23/06/17 12:07:33 INFO FileSourceStrategy: Output Data Schema: struct< age : string,  job  : string,  marital  : string,  education  : string,  default  : string ... 15 more fields>
23/06/17 12:07:33 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 337.4 KiB, free 882.4 MiB)
23/06/17 12:07:33 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 32.3 KiB, free 882.4 MiB)
23/06/17 12:07:33 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on LAPTOP-SPPPB9PB:50700 (size: 32.3 KiB, free: 883.4 MiB)
23/06/17 12:07:33 INFO SparkContext: Created broadcast 3 from show at Demo.scala:19
23/06/17 12:07:33 INFO FileSourceScanExec: Planning scan with bin packing, max size: 9573254 bytes, open cost is considered as scanning 4194304 bytes.
23/06/17 12:07:33 INFO SparkContext: Starting job: show at Demo.scala:19
23/06/17 12:07:33 INFO DAGScheduler: Got job 1 (show at Demo.scala:19) with 1 output partitions
23/06/17 12:07:33 INFO DAGScheduler: Final stage: ResultStage 1 (show at Demo.scala:19)
23/06/17 12:07:33 INFO DAGScheduler: Parents of final stage: List()
23/06/17 12:07:33 INFO DAGScheduler: Missing parents: List()
23/06/17 12:07:33 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[12] at show at Demo.scala:19), which has no missing parents
23/06/17 12:07:33 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 11.6 KiB, free 882.4 MiB)
23/06/17 12:07:33 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 882.4 MiB)
23/06/17 12:07:33 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on LAPTOP-SPPPB9PB:50700 (size: 6.1 KiB, free: 883.4 MiB)
23/06/17 12:07:33 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1478
23/06/17 12:07:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[12] at show at Demo.scala:19) (first 15 tasks are for partitions Vector(0))
23/06/17 12:07:33 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
23/06/17 12:07:33 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (LAPTOP-SPPPB9PB, executor driver, partition 0, PROCESS_LOCAL, 4883 bytes) taskResourceAssignments Map()
23/06/17 12:07:33 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
23/06/17 12:07:33 INFO FileScanRDD: Reading File path: file:///C:/capstone/capstonemarketanalysis/data/Source.csv, range: 0-5378950, partition values: [empty row]
23/06/17 12:07:33 INFO CodeGenerator: Code generated in 99.3739 ms
23/06/17 12:07:33 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2735 bytes result sent to driver
23/06/17 12:07:33 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 310 ms on LAPTOP-SPPPB9PB (executor driver) (1/1)
23/06/17 12:07:33 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
23/06/17 12:07:33 INFO DAGScheduler: ResultStage 1 (show at Demo.scala:19) finished in 0.370 s
23/06/17 12:07:33 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
23/06/17 12:07:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
23/06/17 12:07:33 INFO DAGScheduler: Job 1 finished: show at Demo.scala:19, took 0.382097 s
23/06/17 12:07:34 INFO CodeGenerator: Code generated in 94.2537 ms
+-----+---------------+-----------+------------+----------+----------+----------+-------+----------+------+--------+-----------+-----------+--------+-----------+-----------+---+
| age |          job  |  marital  | education  | default  | balance  | housing  | loan  | contact  | day  | month  | duration  | campaign  | pdays  | previous  | poutcome  |  y|
+-----+---------------+-----------+------------+----------+----------+----------+-------+----------+------+--------+-----------+-----------+--------+-----------+-----------+---+
|  58 |   management  |  married  |  tertiary  |      no  |     2143 |     yes  |   no  | unknown  |    5 |   may  |       261 |         1 |     -1 |         0 |  unknown  | no|
|  44 |   technician  |   single  | secondary  |      no  |       29 |     yes  |   no  | unknown  |    5 |   may  |       151 |         1 |     -1 |         0 |  unknown  | no|
|  33 | entrepreneur  |  married  | secondary  |      no  |        2 |     yes  |  yes  | unknown  |    5 |   may  |        76 |         1 |     -1 |         0 |  unknown  | no|
|  47 |  blue-collar  |  married  |   unknown  |      no  |     1506 |     yes  |   no  | unknown  |    5 |   may  |        92 |         1 |     -1 |         0 |  unknown  | no|
|  33 |      unknown  |   single  |   unknown  |      no  |        1 |      no  |   no  | unknown  |    5 |   may  |       198 |         1 |     -1 |         0 |  unknown  | no|
|  35 |   management  |  married  |  tertiary  |      no  |      231 |     yes  |   no  | unknown  |    5 |   may  |       139 |         1 |     -1 |         0 |  unknown  | no|
|  28 |   management  |   single  |  tertiary  |      no  |      447 |     yes  |  yes  | unknown  |    5 |   may  |       217 |         1 |     -1 |         0 |  unknown  | no|
|  42 | entrepreneur  | divorced  |  tertiary  |     yes  |        2 |     yes  |   no  | unknown  |    5 |   may  |       380 |         1 |     -1 |         0 |  unknown  | no|
|  58 |      retired  |  married  |   primary  |      no  |      121 |     yes  |   no  | unknown  |    5 |   may  |        50 |         1 |     -1 |         0 |  unknown  | no|
|  43 |   technician  |   single  | secondary  |      no  |      593 |     yes  |   no  | unknown  |    5 |   may  |        55 |         1 |     -1 |         0 |  unknown  | no|
|  41 |       admin.  | divorced  | secondary  |      no  |      270 |     yes  |   no  | unknown  |    5 |   may  |       222 |         1 |     -1 |         0 |  unknown  | no|
|  29 |       admin.  |   single  | secondary  |      no  |      390 |     yes  |   no  | unknown  |    5 |   may  |       137 |         1 |     -1 |         0 |  unknown  | no|
|  53 |   technician  |  married  | secondary  |      no  |        6 |     yes  |   no  | unknown  |    5 |   may  |       517 |         1 |     -1 |         0 |  unknown  | no|
|  58 |   technician  |  married  |   unknown  |      no  |       71 |     yes  |   no  | unknown  |    5 |   may  |        71 |         1 |     -1 |         0 |  unknown  | no|
|  57 |     services  |  married  | secondary  |      no  |      162 |     yes  |   no  | unknown  |    5 |   may  |       174 |         1 |     -1 |         0 |  unknown  | no|
|  51 |      retired  |  married  |   primary  |      no  |      229 |     yes  |   no  | unknown  |    5 |   may  |       353 |         1 |     -1 |         0 |  unknown  | no|
|  45 |       admin.  |   single  |   unknown  |      no  |       13 |     yes  |   no  | unknown  |    5 |   may  |        98 |         1 |     -1 |         0 |  unknown  | no|
|  57 |  blue-collar  |  married  |   primary  |      no  |       52 |     yes  |   no  | unknown  |    5 |   may  |        38 |         1 |     -1 |         0 |  unknown  | no|
|  60 |      retired  |  married  |   primary  |      no  |       60 |     yes  |   no  | unknown  |    5 |   may  |       219 |         1 |     -1 |         0 |  unknown  | no|
|  33 |     services  |  married  | secondary  |      no  |        0 |     yes  |   no  | unknown  |    5 |   may  |        54 |         1 |     -1 |         0 |  unknown  | no|
+-----+---------------+-----------+------------+----------+----------+----------+-------+----------+------+--------+-----------+-----------+--------+-----------+-----------+---+
only showing top 20 rows

Exception in thread "main" org.apache.spark.sql.AnalysisException: cannot resolve 'poutcome' given input columns: [mytable. age , mytable. balance  , mytable. campaign  , mytable. contact  , mytable. day  , mytable. default  , mytable. duration  , mytable. education  , mytable. housing  , mytable. job  , mytable. loan  , mytable. marital  , mytable. month  , mytable. pdays  , mytable. poutcome  , mytable. previous  , mytable. y]; line 1 pos 51;
'Aggregate [count(1) AS successCount#136L]
+- 'Filter ('poutcome = success)
   +- SubqueryAlias mytable
      +- View (`mytable`, [ age #16, job  #17, marital  #18, education  #19, default  #20, balance  #21, housing  #22, loan  #23, contact  #24, day  #25, month  #26, duration  #27, campaign  #28, pdays  #29, previous  #30, poutcome  #31, y#32])
         +- Relation [ age #16, job  #17, marital  #18, education  #19, default  #20, balance  #21, housing  #22, loan  #23, contact  #24, day  #25, month  #26, duration  #27, campaign  #28, pdays  #29, previous  #30, poutcome  #31, y#32] csv

	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:54)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(CheckAnalysis.scala:182)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(CheckAnalysis.scala:178)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$2(TreeNode.scala:535)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:535)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$1(TreeNode.scala:532)
	at org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1148)
	at org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1147)
	at org.apache.spark.sql.catalyst.expressions.BinaryExpression.mapChildren(Expression.scala:555)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:532)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsUpWithPruning$1(QueryPlan.scala:181)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:193)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:193)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:204)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:214)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:323)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:214)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUpWithPruning(QueryPlan.scala:181)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:161)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:178)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:97)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:263)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:262)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:262)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:262)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:97)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:92)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:205)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:202)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:75)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:183)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:183)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:75)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:73)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:65)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:618)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:613)
	at Demo$.main(Demo.scala:23)
	at Demo.main(Demo.scala)
23/06/17 12:07:35 INFO SparkContext: Invoking stop() from shutdown hook
23/06/17 12:07:35 INFO SparkUI: Stopped Spark web UI at http://LAPTOP-SPPPB9PB:4040
23/06/17 12:07:35 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/06/17 12:07:35 INFO MemoryStore: MemoryStore cleared
23/06/17 12:07:35 INFO BlockManager: BlockManager stopped
23/06/17 12:07:35 INFO BlockManagerMaster: BlockManagerMaster stopped
23/06/17 12:07:35 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/06/17 12:07:35 INFO SparkContext: Successfully stopped SparkContext
23/06/17 12:07:35 INFO ShutdownHookManager: Shutdown hook called
23/06/17 12:07:35 INFO ShutdownHookManager: Deleting directory C:\Users\kotayaswanth\AppData\Local\Temp\spark-20e374d5-fd19-4fe9-a9e8-ccb83193e87d

Process finished with exit code 1
